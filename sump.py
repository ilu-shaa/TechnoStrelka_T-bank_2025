import re
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict

class NewsProcessor:
    def __init__(self):
        self.model_name = "DeepPavlov/rubert-base-cased"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModel.from_pretrained(self.model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç–∏" –Ω–æ–≤–æ—Å—Ç–µ–π
        # –î–æ–±–∞–≤–ª–µ–Ω—ã –≤–∞–ª—é—Ç–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤
        self.keywords = {
            '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ä—ã–Ω–æ–∫', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–∞–∫—Ü–∏–∏', '–±–∏—Ä–∂–∞',
            '–∏–Ω—Ñ–ª—è—Ü–∏—è', '–∫—Ä–∏–∑–∏—Å', '–≤–∞–ª—é—Ç–∞', '–Ω–µ—Ñ—Ç—å', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            'usd', 'eur', 'rub', 'cny'
        }

    def _split_sentences(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã.
        """
        sentences = re.split(
            r'(?<!\w\.\w.)(?<![A-Z–ê-–Ø][a-z–∞-—è]\.)(?<=[.!?])\s+',
            text
        )
        return [s.strip() for s in sentences if len(s.strip()) > 10]

    def _get_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        """
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        return self._mean_pooling(outputs, inputs['attention_mask']).cpu().numpy()

    def _mean_pooling(self, model_output, attention_mask):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å —É—á–µ—Ç–æ–º –º–∞—Å–∫–∏.
        """
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def _calculate_interest_score(self, text: str) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç–∏" —Ç–µ–∫—Å—Ç–∞ (–∫–∞–∫ –¥–ª—è –≤—Å–µ–π –Ω–æ–≤–æ—Å—Ç–∏, —Ç–∞–∫ –∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
        –°–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ:
          - keyword_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
          - length_score: –æ—Ü–µ–Ω–∫–∞ –ø–æ –¥–ª–∏–Ω–µ (–º–∞–∫—Å–∏–º—É–º = 1)
          - number_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
          - emotion_score: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–∫—Ä–∞—à–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
        –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª ‚Äì –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–∂–Ω–æ –ø–æ–¥–±–∏—Ä–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ).
        """
        # 1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_count = sum(1 for word in text.lower().split() if word in self.keywords)
        # 2. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –¥–æ 1 –ø—Ä–∏ –¥–ª–∏–Ω–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤)
        length_score = min(len(text) / 1000, 1.0)
        # 3. –ù–∞–ª–∏—á–∏–µ —á–∏—Å–µ–ª (—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π)
        number_count = len(re.findall(r'\d+', text))
        # 4. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞
        emotional_words = {'—Ä–æ—Å—Ç', '–ø–∞–¥–µ–Ω–∏–µ', '–∫—Ä–∏–∑–∏—Å', '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–∫–æ—Ä–¥'}
        emotion_score = sum(1 for word in text.lower().split() if word in emotional_words)
        
        return (keyword_count * 0.4 + length_score * 0.2 + 
                number_count * 0.2 + emotion_score * 0.2)

    def _remove_duplicates(self, news_list: List[Dict], threshold: float = 0.85) -> List[Dict]:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        –ï—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–æ–≤–æ—Å—Ç—è–º–∏ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞, —Å—á–∏—Ç–∞–µ—Ç—Å—è, —á—Ç–æ –æ–Ω–∏ –¥—É–±–ª–∏—Ä—É—é—Ç—Å—è.
        """
        if len(news_list) < 2:
            return news_list
            
        embeddings = np.array([item['embedding'] for item in news_list])
        similarity_matrix = cosine_similarity(embeddings)
        
        duplicates = set()
        for i in range(len(similarity_matrix)):
            for j in range(i+1, len(similarity_matrix)):
                if similarity_matrix[i][j] > threshold:
                    duplicates.add(j)
        
        return [item for idx, item in enumerate(news_list) if idx not in duplicates]

    def process_news(self, news_texts: List[str], top_n: int = 5, sentences_in_summary: int = 3) -> List[List[str]]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π:
          1. –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å –ø–æ–¥–±–æ—Ä–æ–º –Ω–∞–∏–±–æ–ª–µ–µ "–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö" –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
             –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:
             - –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
             - –í—ã—á–∏—Å–ª—è–µ–º –±–∞–ª–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
             - –í—ã–±–∏—Ä–∞–µ–º top_K –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –±–∞–ª–ª–æ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ä—è–¥–∫—É, –∫–∞–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ.
          2. –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.
          3. –†–∞—Å—á–µ—Ç "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç–∏" –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏).
          4. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
          5. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç–∏.
          6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ top_n –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å—Ç–æ–≤.
          
        :param news_texts: –°–ø–∏—Å–æ–∫ –∏—Å—Ö–æ–¥–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
        :param top_n: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –æ–¥–Ω–æ–º –ø–æ—Å—Ç–µ.
        :param sentences_in_summary: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –≤–∫–ª—é—á–µ–Ω—ã –≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏.
        :return: –°–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø (–ø–æ—Å—Ç–æ–≤), –≥–¥–µ –∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ ‚Äì —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ —Å –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π.
        """
        processed_news = []
        for text in news_texts:
            sentences = self._split_sentences(text)
            if not sentences:
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–ª–ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentence_scores = [(idx, self._calculate_interest_score(sentence)) 
                               for idx, sentence in enumerate(sentences)]
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –±–∞–ª–ª–æ–º
            top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:sentences_in_summary]
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∏—Ö –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
            top_sentences = sorted(top_sentences, key=lambda x: x[0])
            summary = ' '.join([sentences[idx] for idx, _ in top_sentences])
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            embedding = self._get_embeddings([summary])[0]
            
            processed_news.append({
                'text': summary,
                'embedding': embedding,
                'score': self._calculate_interest_score(summary)
            })
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_news = self._remove_duplicates(processed_news)
        # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç–∏ (–æ—Ç –≤—ã—Å–æ–∫–∏—Ö –±–∞–ª–ª–æ–≤ –∫ –Ω–∏–∑–∫–∏–º)
        sorted_news = sorted(unique_news, key=lambda x: x['score'], reverse=True)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ø–æ—Å—Ç—ã (–∫–∞–∂–¥—ã–π –ø–æ—Å—Ç ‚Äì —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π)
        grouped_news = []
        for i in range(0, len(sorted_news), top_n):
            group = sorted_news[i:i+top_n]
            formatted_group = [
                f"üìå {idx+1}. {item['text']}" 
                for idx, item in enumerate(group)
            ]
            grouped_news.append(formatted_group)
        
        return grouped_news

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è NewsProcessor
    processor = NewsProcessor()
    
    sample_news = [
        '''–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞ –∑–∞–ø—É—Å—Ç–∏–ª–∞ —Ñ—å—é—á–µ—Ä—Å—ã –Ω–∞ –ø—Ä–∏—Ä–æ–¥–Ω—ã–π –≥–∞–∑ Dutch TTF
–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø–∞—Å—ã –≥–∞–∑–∞ –≤ –ï–≤—Ä–æ–ø–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –∑–∞ —Ç—Ä–∏ –≥–æ–¥–∞, —á—Ç–æ –≤ —Ç–æ–º —á–∏—Å–ª–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç—Å—è –Ω–∞ —Ü–µ–Ω–µ —Ç–æ–ø–ª–∏–≤–∞ –Ω–∞ –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏—Ö –±–∏—Ä–∂–∞—Ö. –ì–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–∞–≤–∏—Ç –Ω–∞ –ï–≤—Ä–æ–∑–æ–Ω—É, –∞ –ø–æ–ª–Ω—ã–π –æ—Ç–∫–∞–∑ –æ—Ç —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –≥–∞–∑–∞ –º–æ–∂–µ—Ç –≤–∑–≤–∏–Ω—Ç–∏—Ç—å —Ü–µ–Ω—ã –Ω–∞ –≥–æ—Ä—é—á–µ–µ. –° –ø–æ–º–æ—â—å—é –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –º–æ–∂–Ω–æ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–∞ —Ü–µ–Ω–µ –≥–∞–∑–∞ –≤ –µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–º –≥–∞–∑–æ–≤–æ–º —Ö–∞–±–µ.
DUTCH TTF NATURAL GAS ‚Äî —Å–∞–º–∞—è –∞–∫—Ç–∏–≤–Ω–∞—è –≥–∞–∑–æ–≤–∞—è –±–∏—Ä–∂–∞ –≤ –ï–≤—Ä–æ–ø–µ —Å –æ–±—ä–µ–º–æ–º —Ç–æ—Ä–≥–æ–≤–ª–∏ –æ–∫–æ–ª–æ 20 —Ç—Ä–ª–Ω –∫—É–±–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–æ–≤ —Ç–æ–ø–ª–∏–≤–∞. –ì–æ–ª–ª–∞–Ω–¥—Å–∫–∏–π TTF Gas ‚Äî –≤–µ–¥—É—â–∞—è –µ–≤—Ä–æ–ø–µ–π—Å–∫–∞—è —ç—Ç–∞–ª–æ–Ω–Ω–∞—è —Ü–µ–Ω–∞, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–±—ä–µ–º—ã —Ç–æ—Ä–≥–æ–≤ –±–æ–ª–µ–µ —á–µ–º –≤ 14 —Ä–∞–∑ –ø—Ä–µ–≤—ã—à–∞—é—Ç –æ–±—ä–µ–º—ã –≥–∞–∑–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –ù–∏–¥–µ—Ä–ª–∞–Ω–¥–∞–º–∏ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ü–µ–ª–µ–π.
–†–∞—Å—á–µ—Ç–Ω—ã–µ —Ñ—å—é—á–µ—Ä—Å—ã –Ω–∞ –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–π –≥–∞–∑ –ø–æ–º–æ–≥—É—Ç –±—ã—Å—Ç—Ä–æ –ø–æ–ª—É—á–∏—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞ —Å—á–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω –Ω–∞ —Ç–æ–ø–ª–∏–≤–æ. –ï—Å–ª–∏ –≤—ã —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç–µ, —á—Ç–æ —Ü–µ–Ω—ã –±—É–¥—É—Ç —Å–Ω–∏–∂–∞—Ç—å—Å—è, –∫–æ–Ω—Ç—Ä–∞–∫—Ç –º–æ–∂–Ω–æ —à–æ—Ä—Ç–∏—Ç—å. –ï—Å–ª–∏ –∂–µ —Å—á–∏—Ç–∞–µ—Ç–µ, —á—Ç–æ –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –±—É–¥—É—Ç —Ä–∞—Å—Ç–∏ ‚Äî —Ñ—å—é—á–µ—Ä—Å—ã –º–æ–∂–Ω–æ –∫—É–ø–∏—Ç—å.''',
        '''–£—Ç—Ä–µ–Ω–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç: —Ç–æ—Ä–≥–æ–≤—ã–µ –≤–æ–π–Ω—ã –∏ —Ü–µ–Ω—ã –Ω–∞ –Ω–µ—Ñ—Ç—å, –¥–∞–Ω–Ω—ã–µ –ø–æ –∏–Ω—Ñ–ª—è—Ü–∏–∏ –≤ –†–æ—Å—Å–∏–∏
–ù–∞ —á—Ç–æ —Å—Ç–æ–∏—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —Å–µ–≥–æ–¥–Ω—è, 09.04.2025
–ö–æ–º–ø–∞–Ω–∏–∏:
‚Ä¢ –°–±–µ—Ä–±–∞–Ω–∫: –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –†–ü–ë–£ –∑–∞ –º–∞—Ä—Ç –∏ —Ç—Ä–∏ –º–µ—Å—è—Ü–∞ 2025 –≥–æ–¥–∞.
‚Ä¢ –ê—ç—Ä–æ—Ñ–ª–æ—Ç: –∑–∞—Å–µ–¥–∞–Ω–∏–µ —Å–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤. –ù–∞ –ø–æ–≤–µ—Å—Ç–∫–µ –≤–æ–ø—Ä–æ—Å –≤—ã–ø–ª–∞—Ç—ã –¥–∏–≤–∏–¥–µ–Ω–¥–æ–≤ –∑–∞ 2024 –≥–æ–¥.
–í–∞–ª—é—Ç–Ω—ã–µ –∫—É—Ä—Å—ã (–¶–ë –†–§):
USD/RUB: 85,46 (-0,84%).
EUR/RUB: 93,78 (-1,05%).
CNY/RUB: 11,57 (-1,38%).
–¢–æ–≤–∞—Ä–Ω–æ-—Å—ã—Ä—å–µ–≤–æ–π —Ä—ã–Ω–æ–∫:
Urals: 53,66 (-2,08%).
Brent: 60,20 (-1,73%).
–ó–æ–ª–æ—Ç–æ: 3 014,86 (+1,09%).
–ì–∞–∑: 3,496 (+0,89%).
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:
‚Ä¢ –í —Ö–æ–¥–µ —Ç–æ—Ä–≥–æ–≤ —Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ—Ñ—Ç–∏ –º–∞—Ä–∫–∏ Brent —É–ø–∞–ª–∞ –Ω–∏–∂–µ $61 –∑–∞ –±–∞—Ä—Ä–µ–ª—å –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—Ä—Ç–∞ 2021 –≥–æ–¥–∞. –≠–∫—Å–ø–µ—Ä—Ç—ã —Å–≤—è–∑—ã–≤–∞—é—Ç –¥–∏–Ω–∞–º–∏–∫—É —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ –≤–æ–π–Ω–∞–º–∏ –∏ –º–∏—Ä–æ–≤—ã–º–∏ —Å–∞–Ω–∫—Ü–∏—è–º–∏.''',
        '''–°–±–µ—Ä–±–∞–Ω–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª (https://www.sberbank.ru/ru/sberpress/vazhnoe/article?newsID=3af08b0a-4cfb-4a9e-97aa-e3a0f968bb6f&blockID=8a5ea25e-318c-4d17-a60d-e806c4b0bc07&regionID=22&lang=ru&type=NEWS) –æ—Ç—á–µ—Ç –†–ü–ë–£ –∑–∞ –º–∞—Ä—Ç –∏ –ø–µ—Ä–≤—ã–µ —Ç—Ä–∏ –º–µ—Å—è—Ü–∞ 2025 –≥–æ–¥–∞. –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞—Å—Ç–∏, –∞ —Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–ø–∏—Ç–∞–ª–∞ –≤ –ø–µ—Ä–≤–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 22,6%. –†–æ–∑–Ω–∏—á–Ω—ã–π –∫—Ä–µ–¥–∏—Ç–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å –°–±–µ—Ä–∞ –≤—ã—Ä–æ—Å –Ω–∞ 0,3% –∑–∞ —Å—á–µ—Ç –∏–ø–æ—Ç–µ—á–Ω–æ–≥–æ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è –∏ –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –∫–∞—Ä—Ç. –ù–∞ 1% –ø–æ–¥—Ä–æ—Å –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –∫—Ä–µ–¥–∏—Ç–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å.'''
    ]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ –ø–æ—Å—Ç—ã (–º–∞–∫—Å–∏–º—É–º 5 –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –æ–¥–Ω–æ–º –ø–æ—Å—Ç–µ, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è - 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
    grouped_posts = processor.process_news(sample_news, top_n=5, sentences_in_summary=3)
    
    print("–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏:")
    for idx, group in enumerate(grouped_posts, start=1):
        print(f"\n–ü–æ—Å—Ç {idx}:")
        for line in group:
            print(line)
